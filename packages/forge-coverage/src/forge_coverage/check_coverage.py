#!/usr/bin/env python3
from __future__ import annotations

import argparse
import csv
import hashlib
import json
import logging
import os
import subprocess
import textwrap
import threading
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from html.parser import HTMLParser
from pathlib import Path
from typing import Iterator, Any

import packaging.requirements
import psycopg
import requests
from packaging.tags import Tag
from packaging.utils import parse_wheel_filename, InvalidWheelFilename, canonicalize_name
from packaging.version import Version

SUPPORTED_PYTHON_VERSIONS = ["3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]
SUPPORTED_ARCHITECTURES = ["amd64", "arm64"]
SUPPORTED_MANYLINUX_VARIANTS = ["2_28", "2_39"]

# Flatcover configuration
FLATCOVER_VERSION = "v1.3.0"
FLATCOVER_CHECKSUM = "sha256:1a601b9da1961ee780f532072dda3bfcc637e2f202a47da32ecf1ff89d929495"
# Note: flatcover binaries are Node.js bundles (ncc), platform-independent and identical across architectures
FLATCOVER_URL = f"https://github.com/indexzero/flatlock/releases/download/{FLATCOVER_VERSION}/flatcover-linux-x64"


def parse_args():
    parser = argparse.ArgumentParser("Check built Python wheels against requirements list")
    parser.add_argument("requirements_file", type=Path, nargs='*', help="Path to one or more requirements.txt files for Python modes or package-lock.json files for js mode. Multiple files will be concatenated.")
    parser.add_argument("-I", "--index-url", type=str, default="https://libraries.cgr.dev/python/simple",
                        help="Index URL (Python simple index for index mode, JavaScript registry for js mode)")
    parser.add_argument("-m", "--mode", choices=["index", "db", "sql", "csv", "api", "js"], default="index",
                        help="mode for fetching package information, defaults to %(default)s; "
                        "'index' for fetching packages information from the simple index URL, "
                        "'db' for fetching package information from the database, "
                        "'sql' for generating SQL for offline use, "
                        "'csv' for parsing CSV generated by query from 'sql' mode, "
                        "'api' for interacting with rebuilder-api, "
                        "'js' for checking JavaScript packages from package-lock.json against JavaScript registry")
    parser.add_argument("-D", "--database-url", help="database connection string, defaults to '%(default)s'",
                        default=f"host=localhost port=15432 dbname=ecosystems user={os.getenv('USER', 'first.last')}@chainguard.dev connect_timeout=10")
    parser.add_argument("-v", "--verbose", action="store_true")
    parser.add_argument("-c", "--csv", type=Path, help="path to CSV input file for 'csv' mode")
    parser.add_argument("-g", "--generation", help="look only for specic generation in database")
    parser.add_argument("-a", "--arch", choices=SUPPORTED_ARCHITECTURES,
                        help="require wheels for specific architecture: " + " or ".join(SUPPORTED_ARCHITECTURES))
    parser.add_argument("-V", "--python-version", choices=SUPPORTED_PYTHON_VERSIONS,
                        help="require wheels for specific Python version: " + ", ".join(SUPPORTED_PYTHON_VERSIONS))
    parser.add_argument("-M", "--manylinux-variant", choices=SUPPORTED_MANYLINUX_VARIANTS,
                        help="require wheels for specific manylinux variant: " + " or ".join(SUPPORTED_MANYLINUX_VARIANTS))
    parser.add_argument("-w", "--workers", type=int, default=10,
                        help="number of parallel workers for fetching package information in 'index' mode (default: %(default)s)")
    # API mode arguments
    parser.add_argument("-i", "--issue", type=str, help="GitHub issue number (required for api mode)")
    parser.add_argument("-t", "--token", type=str, help="OIDC token for authentication (defaults to chainctl auth token)")
    parser.add_argument("-u", "--api-url", type=str, default="https://rebuilder-api-python.prod-eco.dev",
                        help="Rebuilder API base URL (default %(default)s)",)
    parser.add_argument("-o", "--organization-id", type=str, help="Organization ID (defaults to chainctl iam organizations list)")
    parser.add_argument("-e", "--environment", choices=["prod", "staging"], default="prod",
                        help="Environment for chainctl token audience (default: %(default)s)")
    parser.add_argument("-r", "--refresh", action="store_true",
                        help="Refresh a request group (api mode only). Re-processes failed/not-found requests.")
    parser.add_argument("-f", "--force", action="store_true",
                        help="Force reprocess all requests in the group (used with --refresh)")
    args = parser.parse_args()
    if args.mode == 'csv' and not args.csv:
        parser.error("the --csv argument is required when mode is 'csv'")
    if args.mode == 'api':
        if not args.issue:
            parser.error("the --issue argument is required when mode is 'api'")
        if args.force and not args.refresh:
            parser.error("the --force argument can only be used with --refresh")
    elif not args.requirements_file or len(args.requirements_file) == 0:
        parser.error("at least one requirements_file is required for modes: index, db, sql, csv, js")
    return args


class LinksParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.a = False
        self.links = []

    def handle_starttag(self, tag: str, attrs):
        if tag == "a":
            self.a = True

    def handle_endtag(self, tag: str):
        if tag == "a":
            self.a = False

    def handle_data(self, data: str):
        if self.a:
            self.links.append(data)


def get_versions(session: requests.Session, package_name: str, index_url: str) -> Iterator[tuple[Version, frozenset[Tag]]]:
    url = f"{index_url.rstrip('/')}/{package_name}/"
    headers = {
        # Seems like GAR blocks requests with python-requests user agent
        "User-Agent": "Chainguard coverage checker",
    }
    response = session.get(url=url, headers=headers, timeout=30, allow_redirects=True)
    response.raise_for_status()
    parser = LinksParser()
    parser.feed(response.text)
    for link in parser.links:
        if not link.endswith(".whl"):
            continue
        try:
            _, version, _, tags = parse_wheel_filename(link)
            yield version, tags
        except InvalidWheelFilename as e:
            logging.warning(f"Invalid wheel filename {link}", exc_info=e)


def load_requirements_from_csv(requirements_file: Path) -> list[packaging.requirements.Requirement]:
    requirements = []
    for line in requirements_file.open(encoding="utf-8").readlines():
        if line.startswith("#"):
            continue
        if not line.strip():
            continue
        requirement = packaging.requirements.Requirement(line.strip())
        requirement.name = canonicalize_name(requirement.name)
        requirements.append(requirement)
    requirements = list(set(requirements))
    requirements.sort(key=lambda r: r.name)
    return requirements


def load_requirements_from_multiple_files(requirements_files: list[Path]) -> list[packaging.requirements.Requirement]:
    """Load and concatenate requirements from multiple files."""
    all_requirements = []
    for requirements_file in requirements_files:
        logging.info(f"Loading requirements from {requirements_file}")
        file_requirements = load_requirements_from_csv(requirements_file)
        all_requirements.extend(file_requirements)
    # Deduplicate and sort
    all_requirements = list(set(all_requirements))
    all_requirements.sort(key=lambda r: r.name)
    logging.info(f"Loaded {len(all_requirements)} unique requirements from {len(requirements_files)} file(s)")
    return all_requirements


def get_chainctl_token(environment: str) -> str:
    """Get OIDC token from chainctl auth token command."""
    try:
        result = subprocess.run(
            ["chainctl", "auth", "token"],
            capture_output=True,
            text=True,
            check=True,
            timeout=30
        )
        token = result.stdout.strip()
        if not token:
            raise ValueError("chainctl returned empty token")
        return token
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Failed to get token from chainctl: {e.stderr}") from e
    except FileNotFoundError:
        raise RuntimeError("chainctl not found in PATH. Please install chainctl.") from None
    except subprocess.TimeoutExpired:
        raise RuntimeError("chainctl auth token timed out after 30 seconds") from None


def get_organization_id(environment: str) -> str:
    """
    Get organization ID from chainctl.
    If multiple orgs exist, error and ask user to specify.
    If single org, return it automatically.
    """
    if environment == "prod":
        audience = "https://console-api.enforce.dev"
    else:
        audience = "https://console-api.chainops.dev"
    try:
        result = subprocess.run(
            ["chainctl", "iam", "organizations", "list", "--audience", audience, "-o", "json"],
            capture_output=True,
            text=True,
            check=True,
            timeout=30
        )
        orgs_output = result.stdout.strip()
        if not orgs_output:
            raise RuntimeError("No organizations found")

        # Parse JSON output
        orgs = json.loads(orgs_output)
        if not isinstance(orgs, dict) or "items" not in orgs:
            raise RuntimeError(f"Unexpected chainctl output format: {orgs_output}")

        org_items = orgs["items"]
        if len(org_items) == 0:
            raise RuntimeError("No organizations found in chainctl output")
        elif len(org_items) == 1:
            org_id = org_items[0].get("id")
            org_name = org_items[0].get("name", "unknown")
            logging.info(f"Auto-selected organization: {org_name} ({org_id})")
            return org_id
        else:
            # Multiple orgs - list them and error
            org_list = "\n".join([f"  - {org.get('name', 'unknown')} ({org.get('id')})" for org in org_items])
            raise RuntimeError(
                f"Multiple organizations found. Please specify one with --organization-id:\n{org_list}"
            )
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Failed to list organizations: {e.stderr}") from e
    except json.JSONDecodeError as e:
        raise RuntimeError(f"Failed to parse chainctl JSON output: {e}") from e
    except subprocess.TimeoutExpired:
        raise RuntimeError("chainctl organizations list timed out after 30 seconds") from None


def build_reference_url(issue: str) -> str:
    """Build GitHub issue reference URL from issue number."""
    return f"https://github.com/chainguard-dev/internal-dev/issues/{issue}"


def chunk_requirements(requirements: list[packaging.requirements.Requirement], chunk_size: int = 2000) -> list[list[packaging.requirements.Requirement]]:
    """Split requirements into chunks of specified size."""
    chunks = []
    for i in range(0, len(requirements), chunk_size):
        chunks.append(requirements[i:i + chunk_size])
    return chunks


def retry_with_backoff(func, max_retries: int = 3, initial_delay: float = 1.0, backoff_factor: float = 2.0):
    """
    Retry a function with exponential backoff.

    Args:
        func: Callable to retry
        max_retries: Maximum number of retry attempts
        initial_delay: Initial delay in seconds
        backoff_factor: Multiplier for delay after each retry

    Returns:
        The result of the function call

    Raises:
        The last exception if all retries fail
    """
    delay = initial_delay
    last_exception = None

    for attempt in range(max_retries + 1):
        try:
            return func()
        except Exception as e:
            last_exception = e
            if attempt < max_retries:
                logging.warning(f"Attempt {attempt + 1}/{max_retries + 1} failed: {e}. Retrying in {delay}s...")
                time.sleep(delay)
                delay *= backoff_factor
            else:
                logging.error(f"All {max_retries + 1} attempts failed.")

    raise last_exception


def append_to_requests_group_api(
    api_url: str, token: str, group_id: int, requirements: list[packaging.requirements.Requirement]
) -> dict:
    """
    Append requests to an existing requests group via API.
    Returns the updated group data.
    """
    # Convert requirements to strings
    requirements_strings = [str(req) for req in requirements]

    payload = {
        "requirements": requirements_strings,
    }

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }

    url = f"{api_url.rstrip('/')}/v1/requests-groups/{group_id}/append"

    try:
        response = requests.post(url, json=payload, headers=headers, timeout=120)
        response.raise_for_status()
        result = response.json()

        added_count = result.get("added_count", 0)
        logging.info(f"Successfully appended {added_count} requests to group {group_id}")

        return result
    except requests.exceptions.HTTPError as e:
        try:
            error_data = e.response.json()
            error_detail = error_data.get("error", str(e))
        except Exception:
            error_detail = str(e)
        raise RuntimeError(f"API append request failed: {error_detail}") from e
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"API append request failed: {e}") from e


def create_requests_group_api(
    api_url: str, token: str, issue: str, requirements: list[packaging.requirements.Requirement],
    organization_id: str, arch: str | None, python_version: str | None, manylinux_variant: str | None
) -> dict:
    """
    Create a new requests group via API.
    Returns the created group data with full request details.
    """
    reference = build_reference_url(issue)

    # Build properties if specified
    properties = None
    if python_version or arch or manylinux_variant:
        properties = {}
        if python_version:
            properties["python_version"] = python_version
        if arch:
            # Convert to API format (amd64 -> x86_64, arm64 -> aarch64)
            api_arch = "x86_64" if arch == "amd64" else "aarch64"
            properties["architecture"] = api_arch
        if manylinux_variant:
            # Convert 2_28 -> 2.28
            properties["glibc_version"] = manylinux_variant.replace("_", ".")

    # Convert requirements to strings
    requirements_strings = [str(req) for req in requirements]

    payload = {
        "reference": reference,
        "requirements": requirements_strings,
        "organization_id": organization_id,
    }
    if properties:
        payload["properties"] = properties

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }

    url = f"{api_url.rstrip('/')}/v1/pypi/requests-groups"

    try:
        # Create the group
        response = requests.post(url, json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        created_group = response.json()

        # Fetch the full group details including requests array
        group_id = created_group.get("id")
        if not group_id:
            raise RuntimeError("API response missing requests group ID")

        get_url = f"{api_url.rstrip('/')}/v1/requests-groups/{group_id}"
        get_response = requests.get(get_url, headers=headers, timeout=30)
        get_response.raise_for_status()
        return get_response.json()
    except requests.exceptions.HTTPError as e:
        try:
            error_data = e.response.json()
            error_detail = error_data.get("error", str(e))
        except Exception:
            error_detail = str(e)
        raise RuntimeError(f"API request failed: {error_detail}") from e
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"API request failed: {e}") from e


def get_requests_groups_by_reference(
    api_url: str, token: str, issue: str, organization_id: str
) -> list[dict]:
    """
    Find requests groups by reference URL.
    Returns list of matching groups (may be empty or have multiple).
    """
    reference = build_reference_url(issue)

    headers = {
        "Authorization": f"Bearer {token}",
    }

    # Use both reference and organization_id filters
    params = {
        "reference": reference,
        "organization_id": organization_id,
    }

    url = f"{api_url.rstrip('/')}/v1/requests-groups"

    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        data = data.get("data", [])
        if not data:
            return []
        groups = []
        for group in data:
            group_id = group.get("id")
            if not group_id:
                raise RuntimeError("API response missing requests group ID")
            id_int = int(group_id)
            request = requests.get(url + f"/{id_int}", headers=headers, timeout=30)
            groups.append(request.json())
        return groups
    except requests.exceptions.HTTPError as e:
        try:
            error_data = e.response.json()
            error_detail = error_data.get("error", str(e))
        except Exception:
            error_detail = str(e)
        raise RuntimeError(f"API request failed: {error_detail}") from e
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"API request failed: {e}") from e


def refresh_requests_group_api(
    api_url: str, token: str, group_id: int, force: bool = False
) -> dict:
    """
    Refresh a requests group via API.
    Re-processes failed/not-found requests, or all requests if force=True.
    Returns the API response.
    """
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }

    params = {}
    if force:
        params["force"] = "true"

    url = f"{api_url.rstrip('/')}/v1/requests-groups/{group_id}/refresh"

    try:
        response = requests.post(url, headers=headers, params=params, timeout=60)
        response.raise_for_status()
        result = response.json()

        message = result.get("message", "Refresh operation completed")
        logging.info(f"Successfully refreshed group {group_id}: {message}")

        return result
    except requests.exceptions.HTTPError as e:
        try:
            error_data = e.response.json()
            error_detail = error_data.get("error", str(e))
        except Exception:
            error_detail = str(e)
        raise RuntimeError(f"API refresh request failed: {error_detail}") from e
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"API refresh request failed: {e}") from e


@dataclass(frozen=True)
class PackageCheckResult:
    requirement: packaging.requirements.Requirement
    status: str  # "found", "no_version", or "not_found"


def check_package(
        session: requests.Session, package_name: str,
        package_requirements: list[packaging.requirements.Requirement],
        index_url: str, arch: str | None, python_version: str | None,
        manylinux_variant: str | None, print_lock: threading.Lock
) -> list[PackageCheckResult]:
    """Check all requirements for a single package."""
    results = []

    # Track which requirements have been satisfied
    unsatisfied = set(package_requirements)
    package_found = False

    with print_lock:
        for req in package_requirements:
            print(f"Checking {req} ...", flush=True)

    try:
        for version, tags in get_versions(session, package_name, index_url):
            package_found = True
            logging.debug("  version %s, tags %s", version, tags)

            # Check each unsatisfied requirement against this version
            for requirement in list(unsatisfied):
                if not requirement.specifier.contains(version, prereleases=True):
                    logging.debug("version %s does not match specifier %s", version, requirement.specifier)
                    continue

                arch_found = True
                if arch is not None:
                    arch_found = False
                    for tag in tags:
                        if tag.platform == "any":
                            arch_found = True
                            break
                        if arch == "amd64" and tag.platform.endswith("x86_64"):
                            arch_found = True
                            break
                        if arch == "arm64" and tag.platform.endswith("aarch64"):
                            arch_found = True
                            break
                if not arch_found:
                    logging.debug("version %s does not have wheel for architecture %s", version, arch)
                    continue

                python_version_found = True
                if python_version is not None:
                    python_version_found = False
                    for tag in tags:
                        if tag.interpreter == "py3" or tag.interpreter == f"cp{python_version.replace('.', '')}":
                            python_version_found = True
                            break

                manylinux_variant_found = True
                if manylinux_variant is not None:
                    manylinux_variant_found = False
                    for tag in tags:
                        if tag.platform == "any" or (
                            # older manylinux versions will also work
                            tag.platform.startswith("manylinux_") and tag.platform.split("_")[1] <= manylinux_variant
                        ):
                            manylinux_variant_found = True
                            break
                if not manylinux_variant_found:
                    logging.debug("version %s does not have wheel for manylinux variant %s", version, manylinux_variant)
                    continue

                if not python_version_found:
                    logging.debug("version %s does not have wheel for Python version %s", version, python_version)
                    continue

                # This version satisfies the requirement!
                logging.debug("version %s matches specifier %s", version, requirement.specifier)
                results.append(PackageCheckResult(requirement, "found"))
                unsatisfied.remove(requirement)

            # If all requirements are satisfied, we can stop early
            if not unsatisfied:
                break

    except Exception as e:
        logging.warning(f"Error checking package {package_name}: {e}")

    # Handle unsatisfied requirements
    for requirement in unsatisfied:
        if package_found:
            results.append(PackageCheckResult(requirement, "no_version"))
        else:
            results.append(PackageCheckResult(requirement, "not_found"))

    return results


def check_coverage_from_index(
        requirements: list[packaging.requirements.Requirement], index_url: str, arch: str | None,
        python_version: str | None, manylinux_variant: str | None, workers: int = 10
) -> None:
    # Group requirements by package name
    packages: dict[str, list[packaging.requirements.Requirement]] = defaultdict(list)
    for req in requirements:
        packages[req.name].append(req)

    found: list[packaging.requirements.Requirement] = []
    no_version: list[packaging.requirements.Requirement] = []
    not_found: list[packaging.requirements.Requirement] = []
    print_lock = threading.Lock()

    with requests.Session() as session:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all package checks
            future_to_package = {
                executor.submit(
                    check_package, session, package_name, package_reqs,
                    index_url, arch, python_version, manylinux_variant, print_lock
                ): package_name
                for package_name, package_reqs in packages.items()
            }

            # Collect results as they complete
            for future in as_completed(future_to_package):
                package_name = future_to_package[future]
                try:
                    results = future.result()
                    for result in results:
                        if result.status == "found":
                            found.append(result.requirement)
                            status_msg = "found"
                        elif result.status == "no_version":
                            no_version.append(result.requirement)
                            status_msg = "no version found"
                        else:
                            not_found.append(result.requirement)
                            status_msg = "package not found"
                        with print_lock:
                            print(f"  {result.requirement} -> {status_msg}")
                except Exception as e:
                    logging.error(f"Error processing package {package_name}: {e}")

    # Check PyPI for packages/versions not found in the index
    not_attempted, not_in_pypi = check_pypi_for_packages(not_found + no_version)

    print(f"Total requirements: {len(requirements)}")
    print(f"Package/version found: {len(found)} ({len(found) * 100 // len(requirements)} %)")
    for r in found:
        print(f"\t{r}")
    print(f"Package/version not found: {len(not_attempted)}  ({len(not_attempted) * 100 // len(requirements)} %)")
    for r in not_attempted:
        print(f"\t{r}")
    print(f"Package/version not found on PyPI: {len(not_in_pypi)}  ({len(not_in_pypi) * 100 // len(requirements)} %)")
    for r in not_in_pypi:
        print(f"\t{r}")


def generate_sql(
        requirements: list[packaging.requirements.Requirement], generation: str | None, arch: str | None,
        python_version: str | None, manylinux_variant: str | None
) -> str:
    packages = list({
        r.name for r in requirements
    })
    if generation and not generation.isalnum():
        raise ValueError("generation must be alphanumeric")
    if arch and not arch in SUPPORTED_ARCHITECTURES:
        raise ValueError("arch must be: " + " or ".join(SUPPORTED_ARCHITECTURES))
    if python_version and not python_version in SUPPORTED_PYTHON_VERSIONS:
        raise ValueError("python_version must be one of:" + ", ".join(SUPPORTED_PYTHON_VERSIONS))
    if manylinux_variant and not manylinux_variant in SUPPORTED_MANYLINUX_VARIANTS:
        raise ValueError("manylinux_variant must be one of: ", ", ".join(SUPPORTED_MANYLINUX_VARIANTS))

    arch_filter = (f"AND (a.properties ->> 'architecture' IS NULL OR a.properties ->> 'architecture' = '{arch}')"
                   if arch is not None else "")
    generation_filter = f"AND a.generation = '{generation}'" if generation is not None else ""
    python_version_filter = (
        f"AND (a.properties ->> 'python_version' IS NULL OR a.properties ->> 'python_version' = '{python_version}')"
        if python_version is not None else ""
    )
    manylinux_variant_filter = (
        f"AND (a.properties ->> 'manylinux_variant' IS NULL OR a.properties ->> 'manylinux_variant' <= '{manylinux_variant}')"
        if manylinux_variant is not None else ""
    )

    return f"""
        WITH args AS (
            {"\nUNION\n".join(["SELECT '" + p + "' AS purl_name" for p in packages])}
        )
        SELECT
            v.purl_name, 
            v.purl_version, 
            (SELECT 
                 COUNT(*) 
             FROM artifact a 
             WHERE 
                a.version_id = v.id 
                AND a.properties ->> 'type' = 'wheel'
                {generation_filter}
                {arch_filter}
                {python_version_filter}
                {manylinux_variant_filter}
            ) AS artifacts_count
        FROM 
            version v INNER JOIN args ON v.purl_name = args.purl_name
        WHERE
            v.purl_type = 'pypi'            
    """


def print_sql(requirements: list[packaging.requirements.Requirement], generation: str | None, arch: str | None,
              python_version: str | None, manylinux_variant: str | None) -> None:
    sql = generate_sql(requirements, generation, arch, python_version, manylinux_variant)
    print(sql)


def check_coverage_from_db(
        requirements: list[packaging.requirements.Requirement], database_url: str, generation: str | None,
        arch: str | None, python_version: str | None, manylinux_variant: str | None
):
    sql = generate_sql(requirements, generation, arch, python_version, manylinux_variant)
    connection = psycopg.connect(database_url)
    with connection.cursor() as cursor:
        cursor.execute(sql)
        rows = cursor.fetchall()
        artifacts_versions = [ArtifactsVersionsItem.from_row(row) for row in rows]
    check_artifacts_versions(requirements, artifacts_versions)


@dataclass(frozen=True)
class ArtifactsVersionsItem:
    package: str
    version: str
    count: int

    @classmethod
    def from_row(cls, row: list[str]) -> ArtifactsVersionsItem:
        return cls(package=row[0], version=row[1], count=int(row[2]))


def check_pypi_for_packages(requirements: list[packaging.requirements.Requirement]) -> tuple[list[packaging.requirements.Requirement], list[packaging.requirements.Requirement]]:
    """
    Check PyPI for the given list of requirements and return two lists:
    - packages/versions found on PyPI
    - packages/versions not found on PyPI
    """
    found: list[packaging.requirements.Requirement] = []
    not_found: list[packaging.requirements.Requirement] = []
    metadata: dict[str, Any] = {}
    with requests.Session() as session:
        for requirement in requirements:
            url = f"https://pypi.org/pypi/{requirement.name}/json"
            if requirement.name not in metadata:
                response = session.get(url=url, timeout=30, allow_redirects=True)
                if response.status_code == 200:
                    metadata[requirement.name] = response.json()
                elif response.status_code == 404:
                    metadata[requirement.name] = None
                else:
                    logging.warning(f"Unexpected status code {response.status_code} for package {requirement.name}")
            requirement_metadata = metadata.get(requirement.name)
            versions = requirement_metadata.get("releases", {}).keys() if requirement_metadata else []
            version_found_in_pypi = False
            for v in versions:
                try:
                    version = Version(v)
                except Exception as e:
                    logging.debug(f"Invalid version {v} for package {requirement.name}", exc_info=e)
                    continue
                if requirement.specifier.contains(version, prereleases=True):
                    logging.debug(f"Version {version} of package {requirement.name} matches specifier {requirement.specifier}")
                    version_found_in_pypi = True
                    break
            if not version_found_in_pypi:
                not_found.append(requirement)
            else:
                found.append(requirement)

    return found, not_found


def check_artifacts_versions(requirements: list[packaging.requirements.Requirement], artifacts_versions: list[ArtifactsVersionsItem]):
    versions_per_package: dict[str, list[ArtifactsVersionsItem]] = {}
    for v in artifacts_versions:
        versions_per_package.setdefault(v.package, []).append(v)
    versions_built: list[packaging.requirements.Requirement] = []
    versions_not_attempted: list[packaging.requirements.Requirement] = []
    versions_failed: list[packaging.requirements.Requirement] = []
    packages_not_attempted: list[packaging.requirements.Requirement] = []

    for r in requirements:
        versions = versions_per_package.get(r.name)
        if not versions:
            packages_not_attempted.append(r)
            continue
        version_attempted = False
        version_build = False
        for artifact_version in versions:
            try:
                version = Version(artifact_version.version)
            except Exception as e:
                logging.warning(f"Invalid version {artifact_version.version} for package {artifact_version.package}", exc_info=e)
                continue
            if r.specifier.contains(version, prereleases=True):
                if artifact_version.count > 0:
                    version_build = True
                    break
                else:
                    version_attempted = True
        if version_build:
            versions_built.append(r)
        elif not version_attempted:
            versions_not_attempted.append(r)
        else:
            versions_failed.append(r)
    not_attempted, not_in_pypi = check_pypi_for_packages(packages_not_attempted + versions_not_attempted)

    print(f"Total requirements: {len(requirements)}")
    print(f"Package/version found: {len(versions_built)} ({len(versions_built) * 100 // len(requirements)} %)")
    for r in versions_built:
        print(f"\t{r}")
    print(f"Package/version not attempted: {len(not_attempted)}  ({len(not_attempted) * 100 // len(requirements)} %)")
    for r in not_attempted:
        print(f"\t{r}")
    print(f"Package/version failed to build: {len(versions_failed)}  ({len(versions_failed) * 100 // len(requirements)} %)")
    for r in versions_failed:
        print(f"\t{r}")
    print(f"Package/version not found on PyPI: {len(not_in_pypi)}  ({len(not_in_pypi) * 100 // len(requirements)} %)")
    for r in not_in_pypi:
        print(f"\t{r}")


def check_coverage_from_csv(requirements: list[packaging.requirements.Requirement], csv_file: Path):
    artifacts_versions: list[ArtifactsVersionsItem] = []
    with csv_file.open(newline="") as csv_file:
        reader = csv.reader(csv_file)
        first_row = next(reader)
        if first_row[0] != "purl_name":
            artifacts_versions.append(ArtifactsVersionsItem.from_row(first_row))
        for row in reader:
            artifacts_versions.append(ArtifactsVersionsItem.from_row(row))
    check_artifacts_versions(requirements, artifacts_versions)


def format_request_status(status: str) -> str:
    """Format request status for display."""
    status_display = {
        "": "pending",
        "found": "found",
        "build_failed": "build failed",
        "no_version": "no version",
        "package_not_found": "package not found",
        "not_in_pypi": "not in PyPI",
        "building": "building",
        "error": "error",
        "excluded": "excluded",
    }
    return status_display.get(status, status)


def format_group_status(status: str) -> str:
    """Format group status for display."""
    return status


def display_group_status(group: dict) -> None:
    """Display status of a single requests group."""
    print(f"\nRequests Group ID: {group['id']}")
    print(f"Reference: {group['reference']}")
    print(f"Status: {format_group_status(group['status'])}")
    print(f"Organization: {group['organization_id']}")
    print(f"Created: {group['created_at']}")
    print(f"Updated: {group['updated_at']}")

    if group.get("properties"):
        print(f"Properties: {json.dumps(group['properties'], indent=2)}")

    requests_list = group.get("requests", [])
    if not requests_list:
        print("No requests found")
        return

    # Count by status
    status_counts = defaultdict(int)
    for req in requests_list:
        status_counts[req["status"]] += 1

    # Display summary
    total = len(requests_list)
    found = status_counts.get("found", 0)
    building = status_counts.get("building", 0)
    pending = status_counts.get("", 0)  # Empty string is pending
    excluded = status_counts.get("excluded", 0)
    not_found = status_counts.get("not_in_pypi", 0)
    effective_total = total - excluded - not_found

    print(f"\nRequests Summary:")
    print(f"      Total: {total}")
    print(f"      Found: {found}/{total} ({found * 100 // total if total > 0 else 0}%)")
    print(f"  Effective: {found}/{effective_total} ({found * 100 // effective_total if effective_total > 0 else 0}%)"
    )
    if building > 0:
        print(f"  Building: {building}")
    if pending > 0:
        print(f"  Pending: {pending}")

    # Group by status for detailed display
    by_status = defaultdict(list)
    for req in requests_list:
        by_status[req["status"]].append(req["requirement"])

    # Display each status group
    for status in [
        "found",
        "building",
        "",
        "no_version",
        "package_not_found",
        "build_failed",
        "not_in_pypi",
        "error",
        "excluded",
    ]:
        if status in by_status:
            reqs = by_status[status]
            status_label = format_request_status(status)
            print(f"\n{status_label.upper()} ({len(reqs)}):")
            for req_str in sorted(reqs):
                print(f"  {req_str}")


# JavaScript-specific functions using flatcover

@dataclass(frozen=True)
class JSPackageResult:
    """Represents a JavaScript package coverage result."""
    name: str
    version: str
    found: bool

    def __str__(self) -> str:
        return f"{self.name}@{self.version}"

    def __hash__(self) -> int:
        return hash((self.name, self.version))


def get_flatcover_cache_dir() -> Path:
    """Get the cache directory for flatcover binary."""
    cache_dir = Path.home() / ".cache" / "check_coverage"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def compute_file_checksum(file_path: Path) -> str:
    """Compute SHA256 checksum of a file."""
    sha256_hash = hashlib.sha256()
    with file_path.open("rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return f"sha256:{sha256_hash.hexdigest()}"


def download_flatcover(cache_dir: Path, url: str, expected_checksum: str) -> Path:
    """
    Download flatcover binary and verify checksum.
    Returns path to the downloaded binary.
    """
    binary_path = cache_dir / f"flatcover-{FLATCOVER_VERSION}.js"

    logging.info(f"Downloading flatcover from {url}")
    try:
        response = requests.get(url, timeout=120)
        response.raise_for_status()

        # Write to temporary file first
        temp_path = binary_path.with_suffix(".tmp")
        temp_path.write_bytes(response.content)

        # Verify checksum
        actual_checksum = compute_file_checksum(temp_path)
        if actual_checksum != expected_checksum:
            temp_path.unlink()
            raise RuntimeError(
                f"Checksum mismatch for flatcover binary. "
                f"Expected {expected_checksum}, got {actual_checksum}"
            )

        # Move to final location and make executable
        temp_path.rename(binary_path)
        binary_path.chmod(0o755)

        logging.info(f"Downloaded and verified flatcover to {binary_path}")
        return binary_path

    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"Failed to download flatcover: {e}") from e


def ensure_flatcover() -> Path:
    """
    Ensure flatcover binary is available.
    Checks for local override, cached version, or downloads new version.
    Returns path to flatcover binary.
    """
    # Check for local override (e.g., locally built version)
    local_flatcover = Path("./flatcover")
    if local_flatcover.exists() and local_flatcover.is_file():
        logging.info(f"Using local flatcover at {local_flatcover}")
        return local_flatcover.absolute()

    # Check for cached version
    cache_dir = get_flatcover_cache_dir()
    cached_binary = cache_dir / f"flatcover-{FLATCOVER_VERSION}.js"

    if cached_binary.exists():
        # Verify checksum of cached version
        try:
            actual_checksum = compute_file_checksum(cached_binary)
            if actual_checksum == FLATCOVER_CHECKSUM:
                logging.info(f"Using cached flatcover at {cached_binary}")
                return cached_binary
            else:
                logging.warning(
                    f"Cached flatcover checksum mismatch. "
                    f"Expected {FLATCOVER_CHECKSUM}, got {actual_checksum}. "
                    f"Redownloading..."
                )
                cached_binary.unlink()
        except Exception as e:
            logging.warning(f"Error verifying cached flatcover: {e}. Redownloading...")
            cached_binary.unlink()

    # Download new version
    return download_flatcover(cache_dir, FLATCOVER_URL, FLATCOVER_CHECKSUM)


def get_javascript_auth_credentials(organization_id: str) -> tuple[str, str]:
    """
    Get Chainguard JavaScript registry authentication credentials using chainctl.
    Returns (identity_id, token) tuple.
    """
    try:
        result = subprocess.run(
            [
                "chainctl", "auth", "pull-token",
                "--repository=javascript",
                f"--parent={organization_id}",
                "--output", "json",
                "--audience", "https://console-api.enforce.dev"
            ],
            capture_output=True,
            text=True,
            check=True,
            timeout=30
        )

        output = result.stdout.strip()

        # Parse JSON output
        try:
            data = json.loads(output)
            identity_id = data.get("identity_id")
            token = data.get("token")
        except json.JSONDecodeError as e:
            raise RuntimeError(
                f"Failed to parse JSON from chainctl output: {e}\nOutput: {output}"
            ) from e

        if not identity_id or not token:
            raise RuntimeError(
                f"Missing identity_id or token in chainctl output: {output}"
            )

        logging.info(f"Retrieved JavaScript auth credentials (identity: {identity_id[:20]}...)")
        return identity_id, token

    except subprocess.CalledProcessError as e:
        raise RuntimeError(
            f"Failed to get JavaScript auth credentials from chainctl: {e.stderr}"
        ) from e
    except FileNotFoundError:
        raise RuntimeError(
            "chainctl not found in PATH. Please install chainctl."
        ) from None
    except subprocess.TimeoutExpired:
        raise RuntimeError(
            "chainctl auth pull-token timed out after 30 seconds"
        ) from None


def run_flatcover(
    flatcover_binary: Path,
    lock_file: Path,
    registry_url: str,
    identity_id: str,
    token: str,
) -> list[JSPackageResult]:
    """
    Run flatcover on a single lock file and return parsed results.
    Streams progress output to the user in real-time.

    Args:
        flatcover_binary: Path to the flatcover binary
        lock_file: Path to the package-lock.json file
        registry_url: URL of the JavaScript registry
        identity_id: Identity ID for authentication
        token: Authentication token
    """
    logging.info(f"Running flatcover on {lock_file}")

    cmd = [
        flatcover_binary,
        lock_file,
        "--cover",
        "--progress",
        "--registry", registry_url,
        "--auth", f"{identity_id}:{token}"
    ]

    # Display the command being run (with masked credentials)
    masked_identity = f"{identity_id[:4]}***{identity_id[-4:]}" if len(identity_id) > 8 else "***"
    masked_token = f"{token[:4]}***{token[-4:]}" if len(token) > 8 else "***"
    print(textwrap.dedent(f"""\
        Command:
          {flatcover_binary} {lock_file} \\
            --cover --progress \\
            --registry {registry_url} \\
            --auth "{masked_identity}:{masked_token}"
        """))

    try:
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            text=True,
        )

        stdout, _ = process.communicate()

        if process.returncode != 0:
            raise subprocess.CalledProcessError(
                process.returncode, cmd,
                output=stdout
            )

        stdout_lines = stdout.splitlines()

        # Parse CSV output
        results = []
        for line in stdout_lines:
            if not line.strip():
                continue
            parts = line.split(",")
            if len(parts) != 3:
                logging.warning(f"Unexpected flatcover output line: {line}")
                continue

            name = parts[0]
            version = parts[1]
            found = parts[2].lower() == "true"

            results.append(JSPackageResult(name=name, version=version, found=found))

        logging.info(f"Processed {len(results)} packages from {lock_file}")
        return results

    except subprocess.CalledProcessError as e:
        raise RuntimeError(
            f"flatcover failed with exit code {e.returncode}:\n"
            f"stdout: {e.stdout}"
        ) from e


def check_js_coverage_with_flatcover(
    lock_files: list[Path],
    registry_url: str,
    environment: str,
    organization_id: str | None,
) -> None:
    """
    Check JavaScript package coverage using flatcover.
    Processes multiple lock files and aggregates results.
    """
    # Ensure flatcover is available

    flatcover_binary = ensure_flatcover()
    # Get organization ID
    if not organization_id:
        logging.info("Auto-detecting organization ID from chainctl")
        organization_id = get_organization_id(environment)
    else:
        logging.info(f"Using provided organization ID: {organization_id}")

    # Get authentication credentials
    logging.info("Getting JavaScript registry authentication credentials")
    identity_id, token = get_javascript_auth_credentials(organization_id)

    # Aggregate results from all files
    all_results: dict[tuple[str, str], bool] = {}  # (name, version) -> found

    for lock_file in lock_files:
        try:
            results = run_flatcover(flatcover_binary, lock_file, registry_url, identity_id, token)

            # Merge results (if same package/version appears in multiple files, use OR logic)
            for result in results:
                key = (result.name, result.version)
                if key in all_results:
                    all_results[key] = all_results[key] or result.found
                else:
                    all_results[key] = result.found

        except Exception as e:
            logging.error(f"Error processing {lock_file}: {e}")
            raise

    # Convert to lists for display
    found = [JSPackageResult(name, version, True)
             for (name, version), is_found in all_results.items() if is_found]
    not_found = [JSPackageResult(name, version, False)
                 for (name, version), is_found in all_results.items() if not is_found]

    # Sort for consistent output
    found.sort(key=lambda p: (p.name, p.version))
    not_found.sort(key=lambda p: (p.name, p.version))

    # Print summary
    total = len(all_results)
    print(f"\nTotal packages: {total}")
    print(f"Packages found: {len(found)} ({len(found) * 100 // total if total > 0 else 0}%)")
    for p in found:
        print(f"\t{p}")
    print(f"Packages not found: {len(not_found)} ({len(not_found) * 100 // total if total > 0 else 0}%)")
    for p in not_found:
        print(f"\t{p}")


def handle_api_mode(args) -> None:
    """
    Handle api mode operation.
    Three sub-modes:
    1. Create mode: --issue <num> --requirements <file> -> creates new group
    2. Status mode: --issue <num> (no requirements) -> shows status
    3. Refresh mode: --issue <num> --refresh -> refreshes existing group
    """
    # Determine if this is create or status mode
    is_create_mode = args.requirements_file is not None and len(args.requirements_file) > 0
    is_refresh_mode = args.refresh

    # Get authentication token
    if args.token:
        token = args.token
        logging.info("Using provided token")
    else:
        logging.info(f"Getting token from chainctl (environment: {args.environment})")
        token = get_chainctl_token(args.environment)

    # Get organization ID
    if args.organization_id:
        organization_id = args.organization_id
        logging.info(f"Using provided organization ID: {organization_id}")
    else:
        logging.info("Auto-detecting organization ID from chainctl")
        organization_id = get_organization_id(args.environment)

    if is_create_mode:
        # CREATE MODE: Load requirements and create new group
        logging.info("CREATE MODE: Creating new requests group")
        requirements = load_requirements_from_multiple_files(args.requirements_file)

        # First check if a group already exists for this issue
        logging.info("Checking if requests group already exists for this issue")
        existing_groups = get_requests_groups_by_reference(
            args.api_url,
            token,
            args.issue,
            organization_id
        )

        if existing_groups:
            print(f"Found existing requests group(s) for issue: {args.issue}")
            print(f"Not creating a new group. Displaying existing group(s):\n")

            for i, group in enumerate(existing_groups):
                if len(existing_groups) > 1:
                    print(f"\n{'='*80}")
                    print(f"GROUP {i+1} of {len(existing_groups)}")
                    print(f"{'='*80}")
                display_group_status(group)
            return

        print(f"Creating requests group for issue: {args.issue}")
        print(f"Total requirements: {len(requirements)}")
        print(f"Organization: {organization_id}")

        # Split requirements into chunks of 2000
        chunks = chunk_requirements(requirements, chunk_size=2000)
        print(f"Split into {len(chunks)} chunk(s) of max 2000 requirements each")

        # Create the first chunk with retry
        print(f"\nCreating group with first chunk ({len(chunks[0])} requirements)...")
        group = retry_with_backoff(
            lambda: create_requests_group_api(
                args.api_url,
                token,
                args.issue,
                chunks[0],
                organization_id,
                args.arch,
                args.python_version,
                args.manylinux_variant
            ),
            max_retries=3
        )
        group_id = group["id"]
        print(f"Successfully created group {group_id}")

        # Append remaining chunks with retry
        if len(chunks) > 1:
            for i, chunk in enumerate(chunks[1:], start=2):
                print(f"\nAppending chunk {i}/{len(chunks)} ({len(chunk)} requirements)...")
                retry_with_backoff(
                    lambda c=chunk: append_to_requests_group_api(
                        args.api_url,
                        token,
                        group_id,
                        c
                    ),
                    max_retries=3
                )
            print(f"\nSuccessfully appended all {len(chunks) - 1} additional chunk(s)")

        # Fetch final group status
        print(f"\nFetching final group status...")
        headers = {"Authorization": f"Bearer {token}"}
        get_url = f"{args.api_url.rstrip('/')}/v1/requests-groups/{group_id}"
        get_response = requests.get(get_url, headers=headers, timeout=30)
        get_response.raise_for_status()
        final_group = get_response.json()

        print(f"\nSuccessfully created and populated requests group:")
        display_group_status(final_group)

    elif is_refresh_mode:
        # REFRESH MODE: Refresh existing group(s)
        logging.info("REFRESH MODE: Refreshing requests group")

        # Find existing groups
        groups = get_requests_groups_by_reference(
            args.api_url,
            token,
            args.issue,
            organization_id
        )

        if not groups:
            print(f"No requests groups found for issue: {args.issue}")
            print(f"Reference URL: {build_reference_url(args.issue)}")
            print(f"Organization: {organization_id}")
            return

        if len(groups) > 1:
            print(f"Found {len(groups)} requests groups for issue: {args.issue}")
            print("ERROR: Multiple groups found. Please specify which group to refresh by group ID.")
            print("Displaying all groups:")
            for i, group in enumerate(groups):
                print(f"\n{'='*80}")
                print(f"GROUP {i+1} of {len(groups)}")
                print(f"{'='*80}")
                display_group_status(group)
            return

        # Single group found, refresh it
        group = groups[0]
        group_id = group["id"]

        print(f"Refreshing requests group {group_id} for issue: {args.issue}")
        if args.force:
            print("Force mode enabled: will reprocess ALL requests")
        else:
            print("Will reprocess failed/not-found requests only")

        # Perform the refresh
        result = retry_with_backoff(
            lambda: refresh_requests_group_api(
                args.api_url,
                token,
                group_id,
                args.force
            ),
            max_retries=3
        )

        print(f"\nRefresh completed: {result.get('message', 'Success')}")

        # Fetch and display updated group status
        print(f"\nFetching updated group status...")
        headers = {"Authorization": f"Bearer {token}"}
        get_url = f"{args.api_url.rstrip('/')}/v1/requests-groups/{group_id}"
        get_response = requests.get(get_url, headers=headers, timeout=30)
        get_response.raise_for_status()
        updated_group = get_response.json()

        print(f"\nUpdated group status:")
        display_group_status(updated_group)

    else:
        # STATUS MODE: Query existing group(s) by reference
        logging.info("STATUS MODE: Querying requests group status")

        groups = get_requests_groups_by_reference(
            args.api_url,
            token,
            args.issue,
            organization_id
        )

        if not groups:
            print(f"No requests groups found for issue: {args.issue}")
            print(f"Reference URL: {build_reference_url(args.issue)}")
            print(f"Organization: {organization_id}")
            return

        if len(groups) > 1:
            print(f"Found {len(groups)} requests groups for issue: {args.issue}")
            print("Displaying all groups:")

        for i, group in enumerate(groups):
            if len(groups) > 1:
                print(f"\n{'='*80}")
                print(f"GROUP {i+1} of {len(groups)}")
                print(f"{'='*80}")
            display_group_status(group)


def main_with_args(args):
    """Main function that accepts pre-parsed arguments (for plugin usage)."""
    if args.mode == "api":
        handle_api_mode(args)
    elif args.mode == "js":
        # JavaScript mode using flatcover
        # For JS mode, default registry URL if not specified
        registry_url = args.index_url if args.index_url != "https://libraries.cgr.dev/python/simple" else "https://libraries.cgr.dev/javascript"
        check_js_coverage_with_flatcover(args.requirements_file, registry_url, args.environment, args.organization_id)
    else:
        # Python modes require requirements_file
        requirements = load_requirements_from_multiple_files(args.requirements_file)
        if args.mode == "index":
            check_coverage_from_index(requirements, args.index_url, arch=args.arch, python_version=args.python_version,
                                      manylinux_variant=args.manylinux_variant, workers=args.workers)
        elif args.mode == "sql":
            print_sql(requirements, args.generation, arch=args.arch, python_version=args.python_version,
                      manylinux_variant=args.manylinux_variant)
        elif args.mode == "db":
            check_coverage_from_db(requirements, args.database_url, generation=args.generation, arch=args.arch,
                                   python_version=args.python_version, manylinux_variant=args.manylinux_variant)
        elif args.mode == "csv":
            check_coverage_from_csv(requirements, args.csv)


def main():
    """Main function for CLI usage."""
    args = parse_args()
    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)
    main_with_args(args)


if __name__ == "__main__":

    main()
